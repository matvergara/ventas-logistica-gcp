# AnÃ¡lisis de Ventas y LogÃ­stica en Google Cloud Platform

Este proyecto implementa una soluciÃ³n analÃ­tica *end-to-end* en ***Google Cloud Platform*** para el anÃ¡lisis de ventas y logÃ­stica, abarcando desde la generaciÃ³n de datos hasta la construcciÃ³n de un ***Data Warehouse*** modelado en esquema estrella.

El objetivo es simular un escenario realista de operaciÃ³n de distribuidores, integrando datos de ventas, stock y clientes, y transformarlos en un repositorio analÃ­tico preparado para consumo en herramientas de ***Business Intelligence***.

---

## ğŸ“š Contexto

El proyecto se basa en un escenario de distribuciÃ³n comercial con mÃºltiples distribuidores, sucursales y clientes, donde se busca responder preguntas operativas y comerciales como:

- EvoluciÃ³n de stock en el tiempo
- DesempeÃ±o en ventas por regiÃ³n y producto
- AnÃ¡lisis de stock y reposiciÃ³n
- ComparaciÃ³n entre distribuidores

> Dado que no se dispone de datos reales, se implementÃ³ un generador de datos sintÃ©ticos en Python que respeta reglas de negocio realistas (condiciones de venta, comportamiento de clientes, reposiciÃ³n de stock, etc.).

## ğŸ§± Arquitectura de la soluciÃ³n
La soluciÃ³n sigue una arquitectura analÃ­tica por capas:
1. GeneraciÃ³n de datos (Python) â†’ 
2. Google Cloud Storage (Data Lake) â†’
3. BigQuery capa raw â†’
4. Data Warehouse â†’
5. Datamarts â†’
6. Dashboard en Looker Studio

## ğŸ§  Decisiones de diseÃ±o

- SeparaciÃ³n por capas para desacoplar ingesta, modelado y consumo
- Uso de esquema estrella para facilitar anÃ¡lisis
- Datamarts para simplificar la lÃ³gica en BI
- SQL versionado y orquestaciÃ³n desde Python


## ğŸ“ Estructura del repositorio
```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_data/        # GeneraciÃ³n de datos sintÃ©ticos
â”‚   â”œâ”€â”€ upload_to_gcs/        # Carga de archivos a Cloud Storage
â”‚   â”œâ”€â”€ load_raw_to_bq/       # Ingesta RAW en BigQuery con control de idempotencia
â”‚   â”œâ”€â”€ dwh/                  # OrquestaciÃ³n del Data Warehouse
â”‚   â”œâ”€â”€ datamarts/            # OrquestaciÃ³n de Datamarts
â”‚   â””â”€â”€ common/               # Utilidades comunes (auth, clientes GCP)
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ dwh/                  # SQL del Data Warehouse (dimensiones y hechos)
â”‚   â””â”€â”€ datamarts/            # SQL de Datamarts
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§® Data Warehouse

El ***Data Warehouse*** estÃ¡ modelado bajo un esquema estrella, separando dimensiones y tablas de hechos.
Dimensiones:
- `dim_fecha`
- `dim_cliente`
- `dim_producto`
- `dim_sucursal`

Hechos:
- `fact_ventas`
- `fact_stock` 

La carga es incremental e idempotente, permitiendo la reejecuciÃ³n del pipeline sin duplicaciÃ³n de datos. 
Se implementa una tabla de control para evitar reprocesos y duplicaciÃ³n de archivos provenientes de ***Cloud Storage***.

## ğŸ“Š Datamarts

Para facilitar el consumo analÃ­tico y simplificar la lÃ³gica en herramientas de BI, se construyÃ³ una capa de ***datamarts*** sobre el *Data Warehouse*.

Los *datamarts* se implementan como vistas en ***BigQuery*** y estÃ¡n orientados al consumo analÃ­tico.

### Datamart de Ventas (`dm_ventas`)
Incluye mÃ©tricas y dimensiones necesarias para el anÃ¡lisis comercial:
- Unidades vendidas
- Importe vendido
- Producto
- Provincia
- Tipo de negocio
- Sucursal
- Dimensiones temporales (aÃ±o, mes, semana ISO)

### Datamart de Stock (`dm_stock`)
Orientado al anÃ¡lisis operativo y logÃ­stico:
- Stock diario por producto y distribuidor
- MÃ©tricas agregables (promedio, mÃ­nimo, desvÃ­o estÃ¡ndar)
- Dimensiones temporales (aÃ±o, mes, semana ISO)

## ğŸ“ˆ Business Intelligence

Los *datamarts* se conectan a ***Looker Studio*** para la visualizaciÃ³n de indicadores clave.

Se desarrollaron *dashboards* con foco en:
- AnÃ¡lisis de ventas
- DistribuciÃ³n geogrÃ¡fica
- DesempeÃ±o por producto y tipo de negocio
- Control de stock y variabilidad

La capa de BI se apoya exclusivamente en los *datamarts*, evitando *joins* y lÃ³gica compleja en la herramienta de visualizaciÃ³n.

[Clickea aquÃ­ para visualizar el Dashboard](https://lookerstudio.google.com/reporting/15bc3841-a472-4f1c-9b98-7ece170edba9/page/N89dF)

## â¡ï¸ EjecuciÃ³n del pipeline
1. GeneraciÃ³n de datos sintÃ©ticos.
```bash
python -m src.generate_data.generate_data
```
2. Carga de archivos en Cloud Storage.
```bash
python -m src.upload_to_gcs.upload_to_gcs
```
3. Ingesta incremental en BigQuery raw.
    a. Creacion de capas raw, dwh y datamarts si no existen:
    ```bash
    python -m src.load_raw_to_bq.setup_datasets
    ```
    b. Creacion de tabla de control
    ```bash
    python -m src.load_raw_to_bq.setup_infra_control
    ```
    c. Ingesta de datos desde Cloud Storage a raw
    ```bash
    python -m src.load_raw_to_bq.load_raw
    ```
4. EjecuciÃ³n del Data Warehouse.
```bash
python -m src.dwh.run_dwh
```
5. EjecuciÃ³n de Datamarts
```bash
python -m src.datamarts.run_datamarts
```

## ğŸ› ï¸ TecnologÃ­as utilizadas
- Python
- Google Cloud Storage
- Google BigQuery
- SQL
- Looker Studio

## ğŸ§‘â€ğŸ’» Autores | Contacto
- **Emanuel Pinasco** â€¢ [LinkedIn](https://www.linkedin.com/in/bruno-inguanzo-974021212/)
- **MatÃ­as Vergara** â€¢ [LinkedIn](https://www.linkedin.com/in/matiasvergaravicencio/)