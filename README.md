# AnÃ¡lisis de Ventas y LogÃ­stica en Google Cloud Platform

Este proyecto implementa una soluciÃ³n analÃ­tica end-to-end en Google Cloud Platform para el anÃ¡lisis de ventas y logÃ­stica, abarcando desde la generaciÃ³n de datos hasta la construcciÃ³n de un Data Warehouse modelado en esquema estrella.

El objetivo es simular un escenario realista de operaciÃ³n de distribuidores, integrando datos de ventas, stock y clientes, y transformarlos en un repositorio analÃ­tico preparado para consumo en herramientas de Business Intelligence.

---

## ğŸ“š Contexto

El proyecto se basa en un escenario de distribuciÃ³n comercial con mÃºltiples distribuidores, sucursales y clientes, donde se busca responder preguntas operativas y comerciales como:

- EvoluciÃ³n de ventas en el tiempo
- DesempeÃ±o por regiÃ³n y sucursal
- AnÃ¡lisis de stock y reposiciÃ³n
- SegmentaciÃ³n de clientes
- ComparaciÃ³n entre distribuidores

> Dado que no se dispone de datos reales, se implementÃ³ un generador de datos sintÃ©ticos en Python que respeta reglas de negocio realistas (condiciones de venta, comportamiento de clientes, reposiciÃ³n de stock, etc.).

## Arquitectura de la soluciÃ³n
La soluciÃ³n sigue una arquitectura analÃ­tica por capas:
1. GeneraciÃ³n de datos (Python) â†’ 
2. Google Cloud Storage (Data Lake) â†’
3. BigQuery capa raw â†’
4. Data Warehouse â†’
5. Datamarts â†’
6. Dashboard en Looker Studio

## ğŸ“ Estructura del repositorio
```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_data/        # GeneraciÃ³n de datos sintÃ©ticos
â”‚   â”œâ”€â”€ upload_to_gcs/        # Carga de archivos a Cloud Storage
â”‚   â”œâ”€â”€ load_raw_to_bq/       # Ingesta RAW en BigQuery con control de idempotencia
â”‚   â”œâ”€â”€ dwh/                  # OrquestaciÃ³n del Data Warehouse
â”‚   â””â”€â”€ common/               # Utilidades comunes (auth, clientes GCP)
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ dwh/                  # SQL del Data Warehouse (dimensiones y hechos)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§® Data Warehouse

El Data Warehouse estÃ¡ modelado bajo un esquema estrella, separando dimensiones y tablas de hechos.
Dimensiones:
- `dim_fecha`
- `dim_cliente`
- `dim_producto`
- `dim_sucursal`

Hechos:
- `fact_ventas`
- `fact_stock` 

La carga es incremental e idempotente, permitiendo la reejecuciÃ³n del pipeline sin duplicaciÃ³n de datos.

### EjecuciÃ³n del pipeline
1. GeneraciÃ³n de datos sintÃ©ticos.
2. Carga de archivos en Cloud Storage.
3. Ingesta incremental en BigQuery raw.
4. EjecuciÃ³n del Data Warehouse.
```bash
python -m src.dwh.run_dwh
```

## ğŸ› ï¸ TecnologÃ­as utilizadas
- Python
- Google Cloud Storage
- Google BigQuery
- SQL
- Looker Studio

## ğŸš§ Estado actual y prÃ³ximos pasos

Actualmente el proyecto cuenta con un pipeline completo hasta la capa de Data Warehouse.

Las prÃ³ximas etapas incluyen:
- ConstrucciÃ³n de datamarts orientados a consumo analÃ­tico
- ReconexiÃ³n y diseÃ±o de dashboards en Looker Studio

## ğŸ§‘â€ğŸ’» Autores | Contacto
- **Emanuel Pinasco** â€¢ [LinkedIn](https://www.linkedin.com/in/bruno-inguanzo-974021212/)
- **MatÃ­as Vergara** â€¢ [LinkedIn](https://www.linkedin.com/in/matiasvergaravicencio/)